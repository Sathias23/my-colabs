{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bradsprigg/prompt-explorer/blob/main/CLIP_Prompt_Synonymiser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2_M3vksZrm1"
      },
      "source": [
        "### This group installs and loads things\n",
        "Open this up if you want to change CLIP's model or some other guts, otherwise just run it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "igvoYYhmRJhp"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install --no-deps git+https://github.com/openai/CLIP.git\n",
        "%pip install --no-deps ftfy regex tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "q36ocM1E18Ir"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import gc\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "def clear_mem():\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0044Oto0Qzm",
        "outputId": "69b35063-09fc-4f81-e378-dfff49ebf9c9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|████████████████████████████████████████| 335M/335M [00:02<00:00, 136MiB/s]\n"
          ]
        }
      ],
      "source": [
        "import clip\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "perceptor, clip_preprocess = clip.load('ViT-B/16')\n",
        "#perceptor, clip_preprocess = clip.load('ViT-B/32')\n",
        "perceptor.eval().float().requires_grad_(False);\n",
        "\n",
        "tokenizer = clip.simple_tokenizer.SimpleTokenizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KMRbY2hyVYII"
      },
      "outputs": [],
      "source": [
        "def similar_words(target_word=\"cool\", topk=32):\n",
        "    target_tokens = tokenizer.encode(target_word)\n",
        "    if len(target_tokens) > 1:\n",
        "        print(\"This word uses more than one token, can't use it!\")\n",
        "        return\n",
        "    target_emb = perceptor.token_embedding.weight[target_tokens[0],None].detach()\n",
        "    token_sim  = torch.cosine_similarity(target_emb,perceptor.token_embedding.weight.detach(),-1)\n",
        "    top_token_sim = torch.topk(token_sim,topk+1,-1,True,True)\n",
        "    top_indices = top_token_sim.indices[1:]\n",
        "    top_values  = top_token_sim.values[1:]\n",
        "    for i in range(top_indices.shape[0]):\n",
        "        print('\"'+tokenizer.decode([top_indices[i].item()])+'\"',\"   \", top_values[i].item())\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RlUjOm_rbgfP"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML, display\n",
        "from random import sample\n",
        "\n",
        "def similar_prompt_fancy(target_word=\"cool\", topk=32):\n",
        "    target_tokens = tokenizer.encode(target_word)\n",
        "    new_prompt = \"\"\n",
        "    for now_token in target_tokens:\n",
        "        target_emb = perceptor.token_embedding.weight[now_token,None].detach()\n",
        "        token_sim  = torch.cosine_similarity(target_emb,perceptor.token_embedding.weight.detach(),-1)\n",
        "        top_token_sim = torch.topk(token_sim,topk+1,-1,True,True)\n",
        "        top_indices = top_token_sim.indices[1:]\n",
        "        top_values  = top_token_sim.values[1:]\n",
        "        output = []\n",
        "        for i in range(top_indices.shape[0]):\n",
        "            output.append([tokenizer.decode([top_indices[i].item()]), top_values[i].item()]) \n",
        "        new_token = sample(output, len(output))[0][0]\n",
        "        new_prompt += new_token\n",
        "    # strip any commas from new_prompt\n",
        "    new_prompt = new_prompt.replace(\", \",\"\")\n",
        "    return new_prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRNAR9XmZ_9t"
      },
      "source": [
        "### Play Area"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmbymJFPaP4K",
        "outputId": "8b53e529-c289-4ea2-82a3-761406639c5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "one grouwith raistevens for perfuon some ground dances party\n",
            "his grouin raiwer onto alloy in the geothermal dances aparty \n",
            "my team , tawer to turbulof a resistance dancingparties \n",
            "one meeting by tares onalloy for some ground dancparties \n",
            "one meeting with rottes of victory of some geothermal dancers dinner \n"
          ]
        }
      ],
      "source": [
        "#@title Prompt Synonyms\n",
        "#@markdown Breaks the prompt down into tokens and then finds similar tokens for each one, selecting one at random \n",
        "#@markdown within topk of each. Concatenates them into a new prompt. Just wanted to see what would happen really.\n",
        "#@markdown Repeats how_many times.\n",
        "\n",
        "prompt = \"a group of ravers on ecstasy at an underground dance party\" #@param {type:\"string\"}\n",
        "topk = 8 #@param {type:\"integer\"}\n",
        "how_many = 5 #@param {type:\"integer\"}\n",
        "output_as_csv_with_original_prompt = False #@param {type:\"boolean\"}\n",
        "\n",
        "#instantiate an array\n",
        "similar_array = []\n",
        "for x in range(0, how_many):\n",
        "    similar = similar_prompt_fancy(prompt, topk)\n",
        "    similar_array.append(similar)\n",
        "    print(similar)\n",
        "#concatenate the array into a string\n",
        "if output_as_csv_with_original_prompt:\n",
        "    #print a new line\n",
        "    print(\"\")\n",
        "    print(\"Original Prompt, Similar Prompt 1, Similar Prompt 2, Similar Prompt 3, Similar Prompt 4, Similar Prompt 5\")\n",
        "    similar_string = prompt + \", \" + \", \".join(similar_array)\n",
        "    print(similar_string)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
