{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bradsprigg/my-colabs/blob/main/Prompt_Explorer_v1_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9VoXqAw_hNY2"
      },
      "source": [
        "# Prompt Explorer v1.3 by Sathias\n",
        "Based on Simple Stable 1.3 by @ai_curio\n",
        "\n",
        "Version History<br>\n",
        "0.1 Quick and Dirty Version<br>\n",
        "0.2 Rewrite based on pharmapsychotic notebook<br>\n",
        "0.3 More configurability options in prompt usage<br>\n",
        "0.4 Added Prompt Randomizer code by @ai_curio as an extra option<br>\n",
        "0.5 Updated to include prompt weights and negative prompts<br>\n",
        "1.0 New prompt creation code and re-written based on Simple Stable 1.2<br>\n",
        "1.1 Added Prompt Mutator<br>\n",
        "1.2 Extra Prompt Mutator options<br>\n",
        "1.3 Rewrite based on Simple Stable 1.3<br>\n",
        "\n",
        "TO-DO\n",
        "*   Logging of Prompts\n",
        "*   Wonderwords integration https://github.com/mrmaxguns/wonderwordsmodule\n",
        "\n",
        "# License\n",
        "This program is free software: you can redistribute it and/or modify\n",
        "it under the terms of the <b>GNU Affero General Public License</b> as\n",
        "published by the Free Software Foundation, either version 3 of the\n",
        "License, or (at your option) any later version.\n",
        "\n",
        "This program is distributed in the hope that it will be useful,\n",
        "but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
        "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
        "GNU Affero General Public License for more details.\n",
        "\n",
        "You should have received a copy of the GNU Affero General Public License\n",
        "along with this program.  If not, see <http://www.gnu.org/licenses/>.\n",
        "\n",
        "![2021-12-03 15_31_06-GNU Affero General Public License v3 (AGPL-3.0) Explained in Plain English - TLD.png](https://media.discordapp.net/attachments/837903223867047946/916428453547474944/2021-12-03_15_31_06-GNU_Affero_General_Public_License_v3_AGPL-3.0_Explained_in_Plain_English_-_TLD.png?width=972&height=540)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxTGQnrCn9RX"
      },
      "source": [
        "# License\n",
        "This program is free software: you can redistribute it and/or modify\n",
        "it under the terms of the <b>GNU Affero General Public License</b> as\n",
        "published by the Free Software Foundation, either version 3 of the\n",
        "License, or (at your option) any later version.\n",
        "\n",
        "This program is distributed in the hope that it will be useful,\n",
        "but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
        "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
        "GNU Affero General Public License for more details.\n",
        "\n",
        "You should have received a copy of the GNU Affero General Public License\n",
        "along with this program.  If not, see <http://www.gnu.org/licenses/>.\n",
        "\n",
        "![2021-12-03 15_31_06-GNU Affero General Public License v3 (AGPL-3.0) Explained in Plain English - TLD.png](https://media.discordapp.net/attachments/837903223867047946/916428453547474944/2021-12-03_15_31_06-GNU_Affero_General_Public_License_v3_AGPL-3.0_Explained_in_Plain_English_-_TLD.png?width=972&height=540)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mr-9Ddq5g1az"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WR73Rwd-oU8n"
      },
      "source": [
        "# Terms of Use\n",
        "\n",
        "Any unauthorized use of these images for profit, monetary gains, and/or commercial means violates the terms of use of this service. \n",
        "\n",
        "If images or their derivatives are presented in a publicly accessible manner in any way, appropriate technological credits (\"Created with Simple Stable\" or thereabouts) **must** also be provided. If multiple images are created and presented as a group by some means, credit is only required for the group, not each individual image. Credit within a broader \"about block\" (\"Bio\", \"About the Artist\", \"Profile\", etc.) also fulfills these requirements. \n",
        "\n",
        "\n",
        "**Due to the nature of the reconfiguring of images generated by Stable Diffusion, the base of this notebook, this service is not copyright friendly. Use of these generated images from copyrighted material can open you to litigation by the copyright holder(s).**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFqfINDOuXgR"
      },
      "source": [
        "# Stuff for nerds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uc5OwvKdjRJF",
        "outputId": "0179ce96-84ce-419f-ab4e-857918a28b67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-78e9d10c-59a7-4d5b-6b07-597c998d3703)\n"
          ]
        }
      ],
      "source": [
        "#@title Check GPU\n",
        "!nvidia-smi -L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PkZgvkHhrgpL"
      },
      "outputs": [],
      "source": [
        "#@title Installation\n",
        "from IPython.display import display, clear_output\n",
        "from os.path import exists\n",
        "import random\n",
        "\n",
        "!pip install gdown\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "outputs_path = \"/content/drive/MyDrive/_output\" #@param {type:\"string\"}\n",
        "!mkdir -p $outputs_path\n",
        "print(f\"Outputs will be saved to {outputs_path}\")\n",
        "\n",
        "!pip install pytorch-lightning==1.7.7 torch-fidelity==0.3.0\n",
        "!pip install numpy==1.21.6 omegaconf==2.2.3 einops==0.5.0 kornia==0.6.8\n",
        "!pip install albumentations==1.2.1 transformers==4.21.3 timm==0.4.12 fairscale==0.4.4\n",
        "!pip install ftfy==6.1.1 jsonmerge==1.8.0 resize-right==0.0.2 torchdiffeq tqdm\n",
        "!pip install torchsde\n",
        "\n",
        "!git clone https://github.com/salesforce/BLIP\n",
        "\n",
        "# Clone Real-ESRGAN and enter the Real-ESRGAN\n",
        "!git clone https://github.com/xinntao/Real-ESRGAN.git\n",
        "%cd Real-ESRGAN\n",
        "# Set up the environment\n",
        "!pip install basicsr\n",
        "!pip install facexlib\n",
        "!pip install gfpgan\n",
        "!pip install -r requirements.txt\n",
        "!python setup.py develop\n",
        "%cd /content\n",
        "\n",
        "!git clone https://github.com/CompVis/stable-diffusion\n",
        "%cd stable-diffusion/\n",
        "!git clone https://github.com/CompVis/taming-transformers\n",
        "!git clone https://github.com/openai/CLIP.git\n",
        "!git clone https://github.com/crowsonkb/k-diffusion.git\n",
        "\n",
        "import sys\n",
        "sys.path.append(\".\")\n",
        "sys.path.append(\"./CLIP\")\n",
        "sys.path.append('./taming-transformers')\n",
        "sys.path.append('./k-diffusion')\n",
        "\n",
        "!echo '' > ./k-diffusion/k_diffusion/__init__.py\n",
        "\n",
        "\n",
        "import argparse, gc, json, os, random, sys, time, glob, requests\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import PIL\n",
        "from contextlib import contextmanager, nullcontext\n",
        "from einops import rearrange, repeat\n",
        "from IPython.display import display, clear_output\n",
        "from itertools import islice\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "from PIL import ImageFilter\n",
        "from PIL import ImageDraw\n",
        "from pytorch_lightning import seed_everything\n",
        "from torch import autocast\n",
        "from ldm.util import instantiate_from_config\n",
        "from ldm.models.diffusion.ddim import DDIMSampler\n",
        "from ldm.models.diffusion.plms import PLMSSampler\n",
        "from k_diffusion.sampling import sample_euler_ancestral\n",
        "from k_diffusion.sampling import sample_lms\n",
        "from k_diffusion.external import CompVisDenoiser\n",
        "\n",
        "!rm -rf /content/stable-diffusion/promptexplorer\n",
        "%cd /content/stable-diffusion\n",
        "!git clone https://github.com/bradsprigg/promptexplorer\n",
        "import promptexplorer.promptexplorer as promptexplorer\n",
        "from promptexplorer.promptmutator import promptmutator\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "class config():\n",
        "    def __init__(self):\n",
        "        self.ckpt = \"/content/sd-weights.ckpt\"\n",
        "        self.config = 'configs/stable-diffusion/v1-inference.yaml'\n",
        "        self.ddim_eta = 0.0\n",
        "        self.ddim_steps = 100\n",
        "        self.fixed_code = True\n",
        "        self.init_img = None\n",
        "        self.n_iter = 1\n",
        "        self.n_samples = 1\n",
        "        self.outdir = \"\"\n",
        "        self.precision = 'autocast'\n",
        "        self.prompt = \"\"\n",
        "        self.sampler = 'klms'\n",
        "        self.scale = 7.5\n",
        "        self.seed = 42\n",
        "        self.strength = 0.75 # strength for noising/unnoising. 1.0 corresponds to full destruction of information in init image\n",
        "        self.H = 512\n",
        "        self.W = 512\n",
        "        self.C = 4\n",
        "        self.f = 8\n",
        "        #upscale stuff\n",
        "        self.passes = 1\n",
        "        self.gobig_overlap = 128\n",
        "        self.detail_steps = 150\n",
        "        self.upscale_strength = 0.3\n",
        "        self.detail_scale = 10\n",
        "\n",
        "def load_model_from_config(config, ckpt, verbose=False):\n",
        "    print(f\"Loading model from {ckpt}\")\n",
        "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
        "    if \"global_step\" in pl_sd:\n",
        "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
        "    sd = pl_sd[\"state_dict\"]\n",
        "    model = instantiate_from_config(config.model)\n",
        "    m, u = model.load_state_dict(sd, strict=False)\n",
        "    if len(m) > 0 and verbose:\n",
        "        print(\"missing keys:\")\n",
        "        print(m)\n",
        "    if len(u) > 0 and verbose:\n",
        "        print(\"unexpected keys:\")\n",
        "        print(u)\n",
        "\n",
        "    model = model.half().to(device)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "def edit_exif_tag_of_image(image_path, tag, value):\n",
        "    image = Image.open(image_path)\n",
        "    exif = image.getexif()\n",
        "    exif[tag] = value\n",
        "    image.save(image_path, exif=exif)\n",
        "\n",
        "opt = config()\n",
        "batch_idx = 0\n",
        "sample_idx = 0\n",
        "clear_output(wait=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3etEC99auas7"
      },
      "source": [
        "# Choose Your Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LUGuqbY_e_5H"
      },
      "outputs": [],
      "source": [
        "#@title Choose Your Model\n",
        "#@markdown Each model will take a few minutes to download when you first select it. To change between models, select another model and run this cell again.\n",
        "#@markdown <br /> If you try to switch models and it doesn't do anything, restart the notebook by Runtime -> Disconnect and Delete Runtime, then Runtime -> Run All\n",
        "#@markdown\n",
        "#@markdown `Stable Diffusion 1.4`: Stable Diffusion 1.4 (Default) <br />\n",
        "#@markdown `Stable Diffusion 1.5`: Stable Diffusion 1.5, requires HuggingFace username and token <br />\n",
        "#@markdown `Pokemon Diffusion`: Pokemon <br />\n",
        "#@markdown `Waifu Diffusion`: Anime style <br />\n",
        "#@markdown `Robo Diffusion`: Robots</p>\n",
        "%cd /content\n",
        "model_choice = 'Open Journey v2' #@param [\"Stable Diffusion 1.4\",\"Stable Diffusion 1.5\", \"Waifu Diffusion\", \"Pokemon Diffusion\", \"Robo Diffusion\", \"Stable Diffusion 1.4 with VAE\", \"Stable Diffusion 1.5 with VAE\", \"Open Journey\", \"Open Journey v2\", \"Dreamlike Diffusion\", \"Dreamlike Photoreal\"] {type:\"string\"}\n",
        "try_google_drive = True #@param {type:\"boolean\"}\n",
        "google_drive_model_location = \"/content/drive/MyDrive/_models\" #@param {type:\"string\"}\n",
        "\n",
        "model_dict = {\n",
        "    \"Stable Diffusion 1.4\": {\n",
        "        \"url\": \"https://huggingface.co/bstddev/sd-v1-4/resolve/main/sd-v1-4.ckpt\",\n",
        "        \"filename\": \"sd-weights-v1-4.ckpt\",\n",
        "        \"requires_hf_login\": False\n",
        "    },\n",
        "    \"Stable Diffusion 1.5\": {\n",
        "        \"url\": \"https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/main/v1-5-pruned-emaonly.ckpt\",\n",
        "        \"filename\": \"sd-weights-v1-5.ckpt\",\n",
        "        \"requires_hf_login\": True\n",
        "    },\n",
        "    \"Pokemon Diffusion\": {\n",
        "        \"url\": \"https://huggingface.co/justinpinkney/pokemon-stable-diffusion/resolve/main/ema-only-epoch%3D000142.ckpt\",\n",
        "        \"filename\": \"pokemon-weights.ckpt\",\n",
        "        \"requires_hf_login\": False\n",
        "    },\n",
        "    \"Waifu Diffusion\": {\n",
        "        \"url\": \"https://huggingface.co/hakurei/waifu-diffusion-v1-3/resolve/main/model-epoch05-float16.ckpt\",\n",
        "        \"filename\": \"waifu-weights.ckpt\",\n",
        "        \"requires_hf_login\": False\n",
        "    },\n",
        "    \"Robo Diffusion\": {\n",
        "        \"url\": \"https://huggingface.co/nousr/robo-diffusion/resolve/main/models/robo-diffusion-v1.ckpt\",\n",
        "        \"filename\": \"robo-weights.ckpt\",\n",
        "        \"requires_hf_login\": False\n",
        "    },\n",
        "    \"Open Journey\": {\n",
        "        \"url\": \"https://huggingface.co/prompthero/openjourney/blob/main/mdjrny-v4.ckpt\",\n",
        "        \"filename\": \"mdjrny-v4.ckpt\",\n",
        "        \"requires_hf_login\": False\n",
        "    },\n",
        "    \"Open Journey v2\": {\n",
        "        \"url\": \"https://huggingface.co/prompthero/openjourney/blob/main/openjourney-v2.ckpt\",\n",
        "        \"filename\": \"openjourney-v2.ckpt\",\n",
        "        \"requires_hf_login\": False\n",
        "    },\n",
        "    \"Dreamlike Diffusion\": {\n",
        "        \"url\": \"https://huggingface.co/dreamlike-art/dreamlike-diffusion-1.0/blob/main/dreamlike-diffusion-1.0.ckpt\",\n",
        "        \"filename\": \"dreamlike-diffusion-1.0.ckpt\",\n",
        "        \"requires_hf_login\": False\n",
        "    },\n",
        "    \"Dreamlike Photoreal\": {\n",
        "        \"url\": \"https://huggingface.co/dreamlike-art/dreamlike-photoreal-1.0/blob/main/dreamlike-photoreal-1.0.ckpt\",\n",
        "        \"filename\": \"dreamlike-photoreal-1.0.ckpt\",\n",
        "        \"requires_hf_login\": False\n",
        "    }\n",
        "}\n",
        "\n",
        "current_model = model_dict[model_choice]\n",
        "model_filename = current_model[\"filename\"]\n",
        "model_url = current_model[\"url\"]\n",
        "\n",
        "if exists(f'{google_drive_model_location}/{model_filename}') and try_google_drive and exists(f'/content/{model_filename}') == False:\n",
        "    print(f\"Copying model from Google Drive\")\n",
        "    !cp \"{google_drive_model_location}/{model_filename}\" /content\n",
        "elif exists(f'/content/{model_filename}') == False:\n",
        "  if current_model[\"requires_hf_login\"]:\n",
        "    print(\"This model requires an authentication token\")\n",
        "    print(\"Please ensure you have accepted its terms of service before continuing.\")\n",
        "\n",
        "    username = input(\"What is your huggingface username?:\")\n",
        "    token = input(\"What is your huggingface token?:\")\n",
        "\n",
        "    _, path = model_url.split(\"https://\")\n",
        "\n",
        "    url = f\"https://{username}:{token}@{path}\"\n",
        "    # contact server for model\n",
        "    print(f\"Attempting to download {model_choice}...this may take a while\")\n",
        "    ckpt_request = requests.get(url)\n",
        "    request_status = ckpt_request.status_code\n",
        "\n",
        "    # inform user of errors\n",
        "    if request_status == 403:\n",
        "      raise ConnectionRefusedError(\"You have not accepted the license for this model.\")\n",
        "    elif request_status == 404:\n",
        "      raise ConnectionError(\"Could not make contact with server\")\n",
        "    elif request_status != 200:\n",
        "      raise ConnectionError(f\"Some other error has ocurred - response code: {request_status}\")\n",
        "\n",
        "    with open(f'/content/{model_filename}', 'wb') as model_file:\n",
        "      model_file.write(ckpt_request.content)\n",
        "    #!wget -O $model_filename $model_url\n",
        "  else:\n",
        "    !wget -O $model_filename $model_url\n",
        "\n",
        "if exists(f'/content/{model_filename}') == False:\n",
        "  print(\"Model download failed, attempting to download default Stable Diffusion:\")\n",
        "  !wget -O sd-weights.ckpt https://huggingface.co/bstddev/sd-v1-4/resolve/main/sd-v1-4.ckpt\n",
        "  model_filename = \"sd-weights.ckpt\"\n",
        "\n",
        "checkpoint_model_file = f'/content/{model_filename}'\n",
        "\n",
        "# #failsafe\n",
        "# list_of_ids = ['1tQIdO4MdKnS-0r4YxZgCdF6OflBGQU7p', '1y4WkObHW16prHFjBMv9ZCtgtXPT32bP0', '1qOhc1zyH8S2NQ5oyo0CArSi28ic66lvp', '1omQKzXWf86EW_NMLnCZNuE91wuEP0LMf', '1hgjezfHowXRyj51P6JXEFjy8jUD5Sy_G', '1b4JgE1GkOif6yeKXxeZhG-Duli239kjP', '1Wgi26blHG4sfGupotlOWW3vdtlE1eoIo', '1ONYa73RsIqLNdBJCrTuzOrdqEMdiyE9q', '1NOTqeURhP9K6hvBT-nfpY8sFglaKJUcq', '1MSX5ZL65LVnLcRejEAkN1dztBv83isV9', '1LQ077sgOifoc8d4xomzgSDH0-X7XRFrp', '19H6H0X-TISCbSlwEdGv49c4UDZ_WqkAo', '11J3j0Hia6e_Fi-cyuOnS4Szlre6ELbp_', '1u8-9d2MdbGRhBc3vE5YeMlhsYspgR032', '1tNS7eC0nIuvhWsErd8PlqlhLh7H5l7dX', '1jMSBAfKp-htSlkQ19T2tu2_a-APWzT0t', '11adl7FCYUVOHGxyZf1Z0uJ86gTx7vPE1', '1Ev_DqVs3f0cI8FZDRxuruBBPJIQAQy8G', '1VbsiAM7gPynKrdo5eECLMUpIa3VxibdD', '1lRjLiHAwFXUt4oy_80DIUx1d3XJJq0pU']\n",
        "# random.shuffle(list_of_ids)\n",
        "# while exists('/content/sd-v1-4.ckpt') == False and len(list_of_ids) > 0:\n",
        "#   print(\"attempting \" + list_of_ids[0])\n",
        "#   !gdown {list_of_ids.pop(0)}\n",
        "\n",
        "# #Failsafe to CDN\n",
        "# if exists('/content/sd-v1-4.ckpt') == False:\n",
        "#   #if random.randint(0, 1) == 0:\n",
        "#   #  !wget https://bearsharktopus.dev/drilbot_pics/sd-v1-4.ckpt\n",
        "#   #else:\n",
        "#   !wget https://bearsharktopus.b-cdn.net/drilbot_pics/sd-v1-4.ckpt\n",
        "# else:\n",
        "#   print(\"Model already downloaded!\")\n",
        "\n",
        "%cd stable-diffusion/\n",
        "opt.ckpt = checkpoint_model_file\n",
        "config = OmegaConf.load(f\"{opt.config}\")\n",
        "model = load_model_from_config(config, f\"{opt.ckpt}\")\n",
        "model = model.to(device)\n",
        "\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "def chunk(it, size):\n",
        "    it = iter(it)\n",
        "    return iter(lambda: tuple(islice(it, size)), ())\n",
        "\n",
        "\"\"\"\n",
        "grabs all text up to the first occurrence of ':' \n",
        "uses the grabbed text as a sub-prompt, and takes the value following ':' as weight\n",
        "if ':' has no value defined, defaults to 1.0\n",
        "repeats until no text remaining\n",
        "\"\"\"\n",
        "def split_weighted_subprompts(text):\n",
        "    remaining = len(text)\n",
        "    prompts = []\n",
        "    weights = []\n",
        "    while remaining > 0:\n",
        "        if \":\" in text:\n",
        "            idx = text.index(\":\") # first occurrence from start\n",
        "            # grab up to index as sub-prompt\n",
        "            prompt = text[:idx]\n",
        "            remaining -= idx\n",
        "            # remove from main text\n",
        "            text = text[idx+1:]\n",
        "            # find value for weight \n",
        "            if \" \" in text:\n",
        "                idx = text.index(\" \") # first occurence\n",
        "            else: # no space, read to end\n",
        "                idx = len(text)\n",
        "            if idx != 0:\n",
        "                try:\n",
        "                    weight = float(text[:idx])\n",
        "                except: # couldn't treat as float\n",
        "                    print(f\"Warning: '{text[:idx]}' is not a value, are you missing a space?\")\n",
        "                    weight = 1.0\n",
        "            else: # no value found\n",
        "                weight = 1.0\n",
        "            # remove from main text\n",
        "            remaining -= idx\n",
        "            text = text[idx+1:]\n",
        "            # append the sub-prompt and its weight\n",
        "            prompts.append(prompt)\n",
        "            weights.append(weight)\n",
        "        else: # no : found\n",
        "            if len(text) > 0: # there is still text though\n",
        "                # take remainder as weight 1\n",
        "                prompts.append(text)\n",
        "                weights.append(1.0)\n",
        "            remaining = 0\n",
        "    return prompts, weights \n",
        "\n",
        "\n",
        "class CFGDenoiser(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.inner_model = model\n",
        "\n",
        "    def forward(self, x, sigma, uncond, cond, cond_scale):\n",
        "        x_in = torch.cat([x] * 2)\n",
        "        sigma_in = torch.cat([sigma] * 2)\n",
        "        cond_in = torch.cat([uncond, cond])\n",
        "        uncond, cond = self.inner_model(x_in, sigma_in, cond=cond_in).chunk(2)\n",
        "        return uncond + (cond - uncond) * cond_scale\n",
        "\n",
        "class config():\n",
        "    def __init__(self):\n",
        "        self.ckpt = checkpoint_model_file\n",
        "        self.config = 'configs/stable-diffusion/v1-inference.yaml'\n",
        "        self.ddim_eta = 0.0\n",
        "        self.ddim_steps = 100\n",
        "        self.fixed_code = True\n",
        "        self.init_img = None\n",
        "        self.n_iter = 1\n",
        "        self.n_samples = 1\n",
        "        self.outdir = \"\"\n",
        "        self.precision = 'autocast'\n",
        "        self.prompt = \"\"\n",
        "        self.sampler = 'klms'\n",
        "        self.scale = 7.5\n",
        "        self.seed = 42\n",
        "        self.strength = 0.75 # strength for noising/unnoising. 1.0 corresponds to full destruction of information in init image\n",
        "        self.H = 512\n",
        "        self.W = 512\n",
        "        self.C = 4\n",
        "        self.f = 8\n",
        "        #upscale stuff\n",
        "        self.passes = 1\n",
        "        self.gobig_overlap = 128\n",
        "        self.detail_steps = 150\n",
        "        self.upscale_strength = 0.3\n",
        "        self.detail_scale = 10\n",
        "      \n",
        "def load_img(path, shape):\n",
        "    if path.startswith('http://') or path.startswith('https://'):\n",
        "        image = Image.open(requests.get(path, stream=True).raw).convert('RGB')\n",
        "    else:\n",
        "        if os.path.isdir(path):\n",
        "            files = [file for file in os.listdir(path) if file.endswith('.png') or file .endswith('.jpg')]\n",
        "            path = os.path.join(path, random.choice(files))\n",
        "            print(f\"Chose random init image {path}\")\n",
        "        image = Image.open(path).convert('RGB')\n",
        "    image = image.resize(shape, resample=PIL.Image.LANCZOS)\n",
        "    image = np.array(image).astype(np.float16) / 255.0\n",
        "    image = image[None].transpose(0, 3, 1, 2)\n",
        "    image = torch.from_numpy(image)\n",
        "    return 2.*image - 1.\n",
        "\n",
        "\n",
        "class BasicSampler():\n",
        "  def __init__(self):\n",
        "    ...\n",
        "  \n",
        "  def sample(opt, c, uc, shape, scale, steps):\n",
        "    raise NotImplementedError()\n",
        "\n",
        "  def sample_img2img(opt, c, uc, shape, init_latent, scale, strength, steps):\n",
        "    raise NotImplementedError()\n",
        "\n",
        "class VanillaSampler(BasicSampler):\n",
        "  def __init__(self, sampler, device):\n",
        "    self.sampler = sampler\n",
        "    self.device = device\n",
        "\n",
        "  def sample(self, opt, c, uc, shape, scale, steps):\n",
        "    self.sampler.make_schedule(ddim_num_steps=steps, ddim_eta=opt.ddim_eta, verbose=False)\n",
        "    x_T = torch.randn([opt.n_samples, opt.C, opt.H // opt.f, opt.W // opt.f], device=self.device)\n",
        "    samples, _ = self.sampler.sample(S=steps,\n",
        "                                 conditioning=c,\n",
        "                                 batch_size=opt.n_samples,\n",
        "                                 shape=shape,\n",
        "                                 verbose=False,\n",
        "                                 unconditional_guidance_scale=scale,\n",
        "                                 unconditional_conditioning=uc,\n",
        "                                 eta=opt.ddim_eta,\n",
        "                                 x_T=x_T)\n",
        "    return samples\n",
        "     \n",
        "  def sample_img2img(self, opt, c, uc, shape, init_latent, scale, strength, steps):\n",
        "    self.sampler.make_schedule(ddim_num_steps=steps, ddim_eta=opt.ddim_eta, verbose=False)\n",
        "    t_enc = int(strength * steps)\n",
        "    z_enc = self.sampler.stochastic_encode(init_latent, torch.tensor([t_enc]*opt.n_samples).to(self.device))\n",
        "    return self.sampler.decode(z_enc, c, t_enc, unconditional_guidance_scale=scale,\n",
        "                             unconditional_conditioning=uc,)\n",
        "\n",
        "class KCrowsonSampler(BasicSampler):\n",
        "  def __init__(self, func, model, device):\n",
        "    self.func = func\n",
        "    self.model_wrap = CompVisDenoiser(model)\n",
        "    self.device = device\n",
        "\n",
        "  def sample(self, opt, c, uc, shape, scale, steps):\n",
        "    sigmas = self.model_wrap.get_sigmas(steps)\n",
        "    model_wrap_cfg = CFGDenoiser(self.model_wrap)\n",
        "    x_T = torch.randn([opt.n_samples, *shape], device=self.device) * sigmas[0]\n",
        "    extra_args = {'cond': c, 'uncond': uc, 'cond_scale': scale}\n",
        "    return self.func(model_wrap_cfg, x_T, sigmas, extra_args=extra_args, disable=False)\n",
        "\n",
        "  def sample_img2img(self, opt, c, uc, shape, init_latent, scale, strength, steps):\n",
        "    t_enc = int(strength * steps)\n",
        "    sigmas = self.model_wrap.get_sigmas(steps)\n",
        "    sigmas_sched = sigmas[steps - t_enc - 1:]\n",
        "    model_wrap_cfg = CFGDenoiser(self.model_wrap)\n",
        "    model_wrap_cfg.init_latent = init_latent\n",
        "    init_latent += torch.randn([opt.n_samples, *shape], device=device) * sigmas_sched[0]\n",
        "    extra_args = {'cond': c, 'uncond': uc, 'cond_scale': scale}\n",
        "    return self.func(model_wrap_cfg, init_latent, sigmas_sched, extra_args=extra_args, disable=False)\n",
        "\n",
        "sampler_dict = {\n",
        "    \"plms\": VanillaSampler(PLMSSampler(model), device),\n",
        "    \"ddim\": VanillaSampler(DDIMSampler(model), device),\n",
        "    \"klms\": KCrowsonSampler(sample_lms, model, device),\n",
        "    \"euler_a\": KCrowsonSampler(sample_euler_ancestral, model, device)\n",
        "} \n",
        "\n",
        "#used by txt2img, img2img and sd upscaling\n",
        "#upscaling uses different scale/strength/steps... and different width/height\n",
        "def generate_samples(opt, init_latent, scale, strength, steps, shape):\n",
        "    data = [opt.n_samples * [opt.prompt]]\n",
        "    precision_scope = autocast if opt.precision == \"autocast\" else nullcontext\n",
        "    sampler = sampler_dict[opt.sampler]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        with precision_scope(\"cuda\"):\n",
        "            with model.ema_scope():\n",
        "                for n in range(opt.n_iter):\n",
        "                    for prompts in data:\n",
        "                        uc = None\n",
        "                        if scale != 1.0:\n",
        "                            uc = model.get_learned_conditioning(opt.n_samples * [\"\"])\n",
        "                        if isinstance(prompts, tuple):\n",
        "                            prompts = list(prompts)\n",
        "\n",
        "                        # weighted sub-prompts\n",
        "                        subprompts,weights = split_weighted_subprompts(prompts[0])\n",
        "                        if len(subprompts) > 1:\n",
        "                            # i dont know if this is correct.. but it works\n",
        "                            c = torch.zeros_like(uc)\n",
        "                            # get total weight for normalizing\n",
        "                            totalWeight = sum(abs(weight) for weight in weights)\n",
        "                            # normalize each \"sub prompt\" and add it\n",
        "                            for i in range(0,len(subprompts)):\n",
        "                                weight = weights[i]\n",
        "                                weight = weight / totalWeight\n",
        "                                c = torch.add(c,model.get_learned_conditioning(subprompts[i]), alpha=weight)\n",
        "                        else: # just standard 1 prompt\n",
        "                            c = model.get_learned_conditioning(prompts)\n",
        "                        \n",
        "                        # negative, copy paste\n",
        "                        # weighted sub-prompts\n",
        "                        if opt.scale != 1.0 and opt.negative != \"\":\n",
        "                            subprompts,weights = split_weighted_subprompts(opt.negative[0])\n",
        "                            if len(subprompts) > 1:\n",
        "                                # i dont know if this is correct.. but it works\n",
        "                                uc = torch.zeros_like(uc)\n",
        "                                # get total weight for normalizing\n",
        "                                totalWeight = sum(abs(weight) for weight in weights)\n",
        "                                # normalize each \"sub prompt\" and add it\n",
        "                                for i in range(0,len(subprompts)):\n",
        "                                    weight = weights[i]\n",
        "                                    weight = weight / totalWeight\n",
        "                                    uc = torch.add(uc,model.get_learned_conditioning(subprompts[i]), alpha=weight)\n",
        "                            else: # just standard 1 prompt\n",
        "                              uc = model.get_learned_conditioning([opt.negative] * opt.n_samples)\n",
        "\n",
        "                        if init_latent != None:\n",
        "                          samples = sampler.sample_img2img(opt, c, uc, shape, init_latent, scale, strength, steps)\n",
        "                        else:\n",
        "                          samples = sampler.sample(opt, c, uc, shape, scale, steps)\n",
        "\n",
        "                        x_samples = model.decode_first_stage(samples)\n",
        "                        x_samples = torch.clamp((x_samples + 1.0) / 2.0, min=0.0, max=1.0)\n",
        "                        return x_samples\n",
        "\n",
        "# for txt2img and img2img\n",
        "def generate(opt):\n",
        "    global sample_idx\n",
        "    seed_everything(opt.seed)\n",
        "    os.makedirs(opt.outdir, exist_ok=True)\n",
        "  \n",
        "    batch_size = opt.n_samples\n",
        "    prompt = opt.prompt\n",
        "    assert prompt is not None\n",
        "    init_latent = None\n",
        "\n",
        "    if opt.init_img != '' and opt.init_img != None:\n",
        "      if opt.sampler == \"plms\":\n",
        "        print (\"Sorry! PLMS doesn't work with img2img, switching to DDIM\")\n",
        "        opt.sampler = \"ddim\"\n",
        "      print(\"calling init image\")\n",
        "      init_image = load_img(opt.init_img, shape=(opt.W, opt.H)).to(device)\n",
        "      init_image = repeat(init_image, '1 ... -> b ...', b=opt.n_samples)\n",
        "      init_latent = model.get_first_stage_encoding(model.encode_first_stage(init_image))  # move to latent space\n",
        "\n",
        "    start_code = None\n",
        "    if opt.fixed_code and init_latent == None:\n",
        "        start_code = torch.randn([opt.n_samples, opt.C, opt.H // opt.f, opt.W // opt.f], device=device)\n",
        "\n",
        "    images = []\n",
        "    filepaths = []\n",
        "\n",
        "    shape = [opt.C, opt.H // opt.f, opt.W // opt.f]\n",
        "    x_samples = generate_samples(opt, init_latent, opt.scale, opt.strength, opt.ddim_steps, shape)\n",
        "\n",
        "    for x_sample in x_samples:\n",
        "        x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
        "        images.append(Image.fromarray(x_sample.astype(np.uint8)))\n",
        "        filepath = os.path.join(opt.outdir, f\"{batch_name}({batch_idx})_{sample_idx:04}.png\")\n",
        "        filepaths.append(filepath)\n",
        "        print(f\"Saving to {filepath}\")\n",
        "        Image.fromarray(x_sample.astype(np.uint8)).save(filepath)\n",
        "        sample_idx += 1\n",
        "    return images, filepaths\n",
        "\n",
        "# Upscaling stuff from https://github.com/jquesnelle/txt2imghd\n",
        "def load_img_for_upscale(img, w, h):\n",
        "    w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n",
        "    image = img.resize((w, h), resample=PIL.Image.LANCZOS)\n",
        "    image = np.array(image).astype(np.float32) / 255.0\n",
        "    image = image[None].transpose(0, 3, 1, 2)\n",
        "    image = torch.from_numpy(image)\n",
        "    return 2.*image - 1.\n",
        "\n",
        "def noisy_ersgan(input: str):\n",
        "    upscale_folder = \"/content/images/ersgan-upscale/\"\n",
        "    escaped_input = shlex.quote(input)\n",
        "    !python /content/Real-ESRGAN/inference_realesrgan.py -n RealESRGAN_x4plus -i $escaped_input -o $upscale_folder\n",
        "\n",
        "    output_path = os.path.join(upscale_folder, f\"{os.path.splitext(os.path.basename(input))[0]}_out{os.path.splitext(os.path.basename(input))[1]}\")\n",
        "    final_output = Image.open(output_path)\n",
        "    final_output = final_output.resize((int(final_output.size[0] / 2), int(final_output.size[1] / 2)), Image.LANCZOS)\n",
        "    return final_output\n",
        "\n",
        "\n",
        "def realesrgan2x(input: str):\n",
        "    return deafen(noisy_ersgan, input)\n",
        "\n",
        "# Upscaling stuff from https://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/master/scripts/sd_upscale.py\n",
        "import math\n",
        "from collections import namedtuple\n",
        "Grid = namedtuple(\"Grid\", [\"tiles\", \"tile_w\", \"tile_h\", \"image_w\", \"image_h\", \"overlap\"])\n",
        "\n",
        "def split_grid(image, tile_w=512, tile_h=512, overlap=64):\n",
        "    w = image.width\n",
        "    h = image.height\n",
        "\n",
        "    non_overlap_width = tile_w - overlap\n",
        "    non_overlap_height = tile_h - overlap\n",
        "\n",
        "    cols = math.ceil((w - overlap) / non_overlap_width)\n",
        "    rows = math.ceil((h - overlap) / non_overlap_height)\n",
        "\n",
        "    dx = (w - tile_w) / (cols - 1) if cols > 1 else 0\n",
        "    dy = (h - tile_h) / (rows - 1) if rows > 1 else 0\n",
        "\n",
        "    grid = Grid([], tile_w, tile_h, w, h, overlap)\n",
        "    for row in range(rows):\n",
        "        row_images = []\n",
        "\n",
        "        y = int(row * dy)\n",
        "\n",
        "        if y + tile_h >= h:\n",
        "            y = h - tile_h\n",
        "\n",
        "        for col in range(cols):\n",
        "            x = int(col * dx)\n",
        "\n",
        "            if x + tile_w >= w:\n",
        "                x = w - tile_w\n",
        "\n",
        "            tile = image.crop((x, y, x + tile_w, y + tile_h))\n",
        "\n",
        "            row_images.append([x, tile_w, tile])\n",
        "\n",
        "        grid.tiles.append([y, tile_h, row_images])\n",
        "\n",
        "    return grid\n",
        "\n",
        "def combine_grid(grid):\n",
        "    def make_mask_image(r):\n",
        "        r = r * 255 / grid.overlap\n",
        "        r = r.astype(np.uint8)\n",
        "        return Image.fromarray(r, 'L')\n",
        "\n",
        "    mask_w = make_mask_image(np.arange(grid.overlap, dtype=np.float32).reshape((1, grid.overlap)).repeat(grid.tile_h, axis=0))\n",
        "    mask_h = make_mask_image(np.arange(grid.overlap, dtype=np.float32).reshape((grid.overlap, 1)).repeat(grid.image_w, axis=1))\n",
        "\n",
        "    combined_image = Image.new(\"RGB\", (grid.image_w, grid.image_h))\n",
        "    for y, h, row in grid.tiles:\n",
        "        combined_row = Image.new(\"RGB\", (grid.image_w, h))\n",
        "        for x, w, tile in row:\n",
        "            if x == 0:\n",
        "                combined_row.paste(tile, (0, 0))\n",
        "                continue\n",
        "\n",
        "            combined_row.paste(tile.crop((0, 0, grid.overlap, h)), (x, 0), mask=mask_w)\n",
        "            combined_row.paste(tile.crop((grid.overlap, 0, w, h)), (x + grid.overlap, 0))\n",
        "\n",
        "        if y == 0:\n",
        "            combined_image.paste(combined_row, (0, 0))\n",
        "            continue\n",
        "\n",
        "        combined_image.paste(combined_row.crop((0, 0, combined_row.width, grid.overlap)), (0, y), mask=mask_h)\n",
        "        combined_image.paste(combined_row.crop((0, grid.overlap, combined_row.width, h)), (0, y + grid.overlap))\n",
        "\n",
        "    return combined_image\n",
        "\n",
        "# suppress output from one function\n",
        "import shlex\n",
        "def deafen(function, *args):\n",
        "    real_stdout = sys.stdout\n",
        "    sys.stdout = open(os.devnull, \"w\")\n",
        "    output = function(*args)\n",
        "    sys.stdout = real_stdout\n",
        "    return output\n",
        "\n",
        "\n",
        "#upscale\n",
        "def to_big(filepaths, opt):\n",
        "\n",
        "    batch_size = opt.n_samples\n",
        "    prompt = opt.prompt\n",
        "    assert prompt is not None\n",
        "    data = [batch_size * [prompt]]\n",
        "    tile_w = 512\n",
        "    tile_h = 512\n",
        "    u_filepaths = []\n",
        "    u_images = []\n",
        "    precision_scope = autocast if opt.precision == \"autocast\" else nullcontext\n",
        "\n",
        "    for base_filename in filepaths:\n",
        "        for _ in range(opt.passes):\n",
        "            filename = os.path.splitext(base_filename)[0]\n",
        "            \n",
        "            source_image = realesrgan2x(base_filename)\n",
        "            og_size = (opt.H,opt.W)\n",
        "            grid = split_grid(source_image, tile_w=tile_w, tile_h=tile_h, overlap=64)\n",
        "\n",
        "            work = []\n",
        "\n",
        "            for y, h, row in grid.tiles:\n",
        "              for tiledata in row:\n",
        "                work.append(tiledata[2])\n",
        "\n",
        "            batch_count = math.ceil(len(work) / batch_size)\n",
        "\n",
        "            work_results = []\n",
        "            for i in range(batch_count):\n",
        "              chunk = work[i*batch_size:(i+1)*batch_size]\n",
        "              init_image =  load_img_for_upscale(chunk[0], tile_w, tile_h).to(torch.half).to(device)\n",
        "              init_image = repeat(init_image, '1 ... -> b ...', b=batch_size)\n",
        "              init_latent = model.get_first_stage_encoding(model.encode_first_stage(init_image))\n",
        "\n",
        "              shape = [opt.C, tile_h // opt.f, tile_w // opt.f]\n",
        "              x_samples = deafen(generate_samples, opt, init_latent, opt.detail_scale, opt.upscale_strength, opt.ddim_steps, shape)\n",
        "\n",
        "              for x_sample in x_samples:\n",
        "                  x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
        "                  resultslice = Image.fromarray(x_sample.astype(np.uint8)).convert('RGBA')\n",
        "                  work_results.append((resultslice.copy()))\n",
        "              \n",
        "            image_index = 0\n",
        "            for y, h, row in grid.tiles:\n",
        "                for tiledata in row:\n",
        "                    tiledata[2] = work_results[image_index] if image_index < len(work_results) else Image.new(\"RGB\", (tile_w, tile_h))\n",
        "                    image_index += 1\n",
        "\n",
        "            final_output = combine_grid(grid)\n",
        "            final_output.save(f\"{filename}d.png\")\n",
        "        \n",
        "        filename = f\"{filename}_u.png\"\n",
        "        final_output.save(filename)\n",
        "        u_filepaths.append(filename)\n",
        "        u_images.append(final_output)\n",
        "\n",
        "    return u_images, u_filepaths\n",
        "\n",
        "clear_output(wait=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoXhUDDXo1nU"
      },
      "source": [
        "# Image creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "wzmVAdZ1-5tE",
        "outputId": "8e7d1dc3-9eef-4849-a987-da717a784ea1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:pytorch_lightning.utilities.seed:Global seed set to 820615087\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating: None\n",
            "Minus: None\n"
          ]
        },
        {
          "ename": "AssertionError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-b4c83df816ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mndist\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Distance: {ndist}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m     \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Used seed: {opt.seed}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-47b60fc8f37c>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(opt)\u001b[0m\n\u001b[1;32m    368\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m     \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m     \u001b[0minit_latent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#!rm -rf /content/images/*\n",
        "#@title Put your stuff here!\n",
        "from datetime import datetime\n",
        "batch_name = \"20221217_OPv2_test\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown <b>Prompt Explorer Settings</b>\n",
        "#@markdown ---\n",
        "COMBINE_METHOD = \"Directional\" #@param [\"No weights\", \"Directional\", \"Constant Weights\"]\n",
        "SEGMENTS_1 = \"\" #@param {type:\"string\"}\n",
        "SEGMENT_COUNT_1 = 0 #@param {type:\"slider\", min:0, max:5, step:1} \n",
        "SEGMENT_WEIGHT_1 = 1.0 #@param {type:\"number\"}\n",
        "SEGMENTS_2 = \"weird and disgusting [[undead]] [[creature]] with [[horrifying]] [[fangs]], [[zombie]] [[horde]] in a post-apocalyptic dystopian city, voodoo witch-doctor in his [[secret]] [[magic]] lair casting a [[curse]], [[mexican]] [[undead]] gunslinger versed in the art of dia dos muertos, horrifying [[swamp witch]] collecting [[body]] parts , an immortal [[lich]] holding a [[magical orb]] surrounded by an [[aura of darkness]]\" #@param {type:\"string\"}\n",
        "SEGMENT_COUNT_2 = 1 #@param {type:\"slider\", min:0, max:5, step:1} \n",
        "SEGMENT_WEIGHT_2 = 1.0 #@param {type:\"number\"}\n",
        "SEGMENTS_3 = \"horror digital illustration, horror concept art, horror surrealism, creepy glitch art, zombiecore digital render, horror fantasy concept art\" #@param {type:\"string\"}\n",
        "SEGMENT_COUNT_3 = 1 #@param {type:\"slider\", min:0, max:5, step:1} \n",
        "SEGMENT_WEIGHT_3 = 0.3 #@param {type:\"number\"}\n",
        "SEGMENTS_4 = \"Zdzislaw Beksinski, artgerm, beeple, HR Giger, Magali Villeneuve, Stephan Martiniere, Dusan Markovic, Pat Presly, Jaros\\u0142aw Ja\\u015Bnikowski, Peter Gric, Ross Tran, Loic Zimmerman, Tim White, Wayne Barlowe, Jim Burns, Igor Morski, Stephan Koidl, Norman Rockwell\" #@param {type:\"string\"}\n",
        "SEGMENT_COUNT_4 = 3 #@param {type:\"slider\", min:0, max:5, step:1} \n",
        "SEGMENT_WEIGHT_4 = 0.6 #@param {type:\"number\"}\n",
        "SEGMENTS_5 = \"freakshow, nightmare, ominous, creepy, creepypasta, dark, mysterious, occult, killer, monster, paranormal, psychological horror, Devilcore, gorecore, gothic, Gurokawa, Halloween, Witchcore, voodoo, ornament, super-resolution, dark design, dark lighting, neo-gothic, hyper detailed, neoplasticism, cosmic horror, grotesque, lovecraftian, flickering light, nightmare\" #@param {type:\"string\"}\n",
        "SEGMENT_COUNT_5 = 5 #@param {type:\"slider\", min:0, max:5, step:1} \n",
        "SEGMENT_WEIGHT_5 = 0.8 #@param {type:\"number\"}\n",
        "SEGMENTS_6 = \"4k, sharp focus, trending on artstation, trending on deviant art, octane engine, unreal engine, trending on cg society, 8k, soft lighting, ray tracing global illumination, in a symbolic and meaningful style, digital painting, procreate, depth perception, depth of field\" #@param {type:\"string\"}\n",
        "SEGMENT_COUNT_6 = 3 #@param {type:\"slider\", min:0, max:5, step:1} \n",
        "SEGMENT_WEIGHT_6 = 0.5 #@param {type:\"number\"}\n",
        "SEGMENTS_7 = \"duplicate, people or animals, natural light sources, familiar objects or landscapes, recognizable patterns or structures, colors associated with warmth or comfort, recognizable forms of life, recognizable physical laws or rules, sense of time or space, sense of stability or security, sense of hope or salvation, blurry, Happiness, Love, Joy, Safety, Comfort, Peace, Brightness, Warmth, Laughter, Serenity\" #@param {type:\"string\"}\n",
        "SEGMENT_COUNT_7 = 5 #@param {type:\"slider\", min:0, max:5, step:1} \n",
        "SEGMENT_WEIGHT_7 = -0.5 #@param {type:\"number\"}\n",
        "prompt = \"\"\n",
        "negative = \"\"\n",
        "\n",
        "#@markdown <br>\n",
        "#@markdown <b>Prompt Mutator</b><br>\n",
        "#@markdown The Prompt Mutator will take the generated prompt and use CLIP to randomise the tokens to another one within <i>distance</i>\n",
        "#@markdown <br>\n",
        "use_prompt_mutator = \"Specific Words\" #@param [\"Off\", \"Entire Prompt\", \"Specific Words\"] {type:\"string\"}\n",
        "distance = 10 #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "#convert distance to integer\n",
        "distance = int(distance)\n",
        "\n",
        "#@markdown <br>\n",
        "#@markdown <b>Image Settings</b><br>\n",
        "width_height = [512, 768] #@param{type: 'raw'}\n",
        "#@markdown <br>\n",
        "steps = 40 #@param {type:\"integer\"}\n",
        "samples_per_batch = 1 #@param {type:\"integer\"}\n",
        "batches = 200 #@param {type:\"integer\"}\n",
        "seed = -1 #@param {type:\"integer\"}\n",
        "sampler = 'euler_a' #@param [\"ddim\", \"plms\", \"klms\", \"euler_a\"] {type:\"string\"}\n",
        "#batch_name = batch_name + datetime.now().strftime(\"%H:%M:%S\")\n",
        "guidance_scale = 10 #@param {type:\"number\"}\n",
        "ddim_eta = 0.0\n",
        "zoom = False\n",
        "\n",
        "#@markdown <br>\n",
        "#@markdown\n",
        "#@markdown <b>Init image</b><br>\n",
        "#@markdown `init_image_or_folder`: url or path to an image, or path to a folder to pick random images from<br>\n",
        "#@markdown `init_strength`: from 0.0 to 1.0 how much the init image is used<br>\n",
        "init_image_or_folder = \"\" #@param {type:\"string\"}\n",
        "init_strength = 0.35 #@param {type:\"slider\", min:0.0, max:1.0, step:0.05}\n",
        "#@markdown <br>\n",
        "#@markdown\n",
        "#@markdown <b>Upscaling</b><br>\n",
        "#@markdown `upscale_results`: tick this to upscale each image, will take longer to generate <br />\n",
        "#@markdown `detailing`: controls the upscaling steps, higher is more detailed but takes longer and might introduce weird details <br />\n",
        "#@markdown `upscale_strength`: like init_strength but for upscaling, 0.7 is default\n",
        "upscale_results = False #@param {type: 'boolean'}\n",
        "detailing = 1 #@param {type:\"slider\", min:1, max:4, step:1}\n",
        "upscale_strength = 0.7 #@param {type:\"slider\", min:0.0, max:1.0, step:0.05}\n",
        "\n",
        "opt.init_img = init_image_or_folder\n",
        "opt.ddim_steps = steps\n",
        "opt.n_iter = 1\n",
        "opt.n_samples = samples_per_batch\n",
        "opt.outdir = os.path.join(outputs_path, batch_name)\n",
        "opt.prompt = prompt\n",
        "opt.negative = negative\n",
        "opt.sampler = sampler\n",
        "opt.scale = guidance_scale\n",
        "opt.seed = random.randint(0, 2**32) if seed == -1 else seed\n",
        "opt.strength = max(0.0, min(1.0, 1.0 - init_strength))\n",
        "opt.W, opt.H = map(lambda x: x - x % 64, (width_height[0], width_height[1])) # resize to integer multiple of 64\n",
        "#upscale stuff\n",
        "opt.upscale = upscale_results\n",
        "opt.passes = 1\n",
        "opt.gobig_overlap = 128\n",
        "#opt.detail_steps = 150\n",
        "opt.detail_steps = steps*detailing\n",
        "opt.upscale_strength = max(0.0, min(1.0, 1.0 - upscale_strength))\n",
        "opt.detail_scale = 10\n",
        "\n",
        "if opt.strength >= 1 or init_image_or_folder == None:\n",
        "    opt.init_img = \"\"\n",
        "\n",
        "if opt.sampler != 'ddim':\n",
        "    opt.ddim_eta = 0.0\n",
        "\n",
        "# save settings\n",
        "settings = {\n",
        "    'ddim_eta': ddim_eta,\n",
        "    'guidance_scale': guidance_scale,\n",
        "    'init_image': init_image_or_folder,\n",
        "    'init_strength': init_strength,\n",
        "    'num_batch_images': 1,\n",
        "    'prompt': prompt,\n",
        "    'sampler': sampler,\n",
        "    'samples_per_batch': samples_per_batch,\n",
        "    'seed': opt.seed,\n",
        "    'steps': steps,\n",
        "    'width': opt.W,\n",
        "    'height': opt.H,\n",
        "}\n",
        "os.makedirs(opt.outdir, exist_ok=True)\n",
        "while os.path.isfile(f\"{opt.outdir}/{batch_name}({batch_idx})_settings.txt\"):\n",
        "    batch_idx += 1\n",
        "with open(f\"{opt.outdir}/{batch_name}({batch_idx})_settings.txt\", \"w+\", encoding=\"utf-8\") as f:\n",
        "    json.dump(settings, f, ensure_ascii=False, indent=4)\n",
        "sample_idx = 0\n",
        "\n",
        "promptFragments = promptexplorer.promptFragments()\n",
        "promptFragments.clear()\n",
        "# only add fragments if they are not empty\n",
        "if SEGMENT_COUNT_1 > 0:\n",
        "    promptFragments.addFragment(SEGMENTS_1, SEGMENT_COUNT_1, SEGMENT_WEIGHT_1)\n",
        "if SEGMENT_COUNT_2 > 0:\n",
        "    promptFragments.addFragment(SEGMENTS_2, SEGMENT_COUNT_2, SEGMENT_WEIGHT_2)\n",
        "if SEGMENT_COUNT_3 > 0:\n",
        "    promptFragments.addFragment(SEGMENTS_3, SEGMENT_COUNT_3, SEGMENT_WEIGHT_3)\n",
        "if SEGMENT_COUNT_4 > 0:\n",
        "    promptFragments.addFragment(SEGMENTS_4, SEGMENT_COUNT_4, SEGMENT_WEIGHT_4)\n",
        "if SEGMENT_COUNT_5 > 0:\n",
        "    promptFragments.addFragment(SEGMENTS_5, SEGMENT_COUNT_5, SEGMENT_WEIGHT_5)\n",
        "if SEGMENT_COUNT_6 > 0:\n",
        "    promptFragments.addFragment(SEGMENTS_6, SEGMENT_COUNT_6, SEGMENT_WEIGHT_6)\n",
        "if SEGMENT_COUNT_7 > 0:\n",
        "    promptFragments.addFragment(SEGMENTS_7, SEGMENT_COUNT_7, SEGMENT_WEIGHT_7)\n",
        "\n",
        "hybrid_index = 0\n",
        "\n",
        "for i in range(batches):\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    pdist = 0\n",
        "    ndist = 0\n",
        "    if COMBINE_METHOD == \"No weights\":\n",
        "        prompt = promptFragments.combineFragments(promptexplorer.CombineMethod.SELECT_NUM)\n",
        "        negative = \"\"\n",
        "    if COMBINE_METHOD == \"Directional\":\n",
        "        prompt = promptFragments.combineFragments(promptexplorer.CombineMethod.SELECT_NUM_DIRECTIONAL, promptexplorer.Direction.POSITIVE)\n",
        "        negative = promptFragments.combineFragments(promptexplorer.CombineMethod.SELECT_NUM_DIRECTIONAL, promptexplorer.Direction.NEGATIVE)\n",
        "    elif COMBINE_METHOD == \"Constant Weights\":\n",
        "        prompt = promptFragments.combineFragments(promptexplorer.CombineMethod.SELECT_NUM_WITH_WEIGHT, promptexplorer.Direction.POSITIVE)\n",
        "        negative = promptFragments.combineFragments(promptexplorer.CombineMethod.SELECT_NUM_WITH_WEIGHT, promptexplorer.Direction.NEGATIVE)\n",
        "    opt.prompt = prompt\n",
        "    opt.negative = negative\n",
        "    if use_prompt_mutator == \"Entire Prompt\":\n",
        "        syn = promptmutator(prompt, distance)\n",
        "        prompt, pdist = syn.synonymise()\n",
        "        opt.prompt = prompt\n",
        "        syn = promptmutator(negative, distance)\n",
        "        negative, ndist = syn.synonymise()\n",
        "        opt.negative = negative\n",
        "    if use_prompt_mutator == \"Specific Words\":\n",
        "        syn = promptmutator(prompt, distance)\n",
        "        prompt = syn.mutate_specific_words()\n",
        "        opt.prompt = prompt\n",
        "        syn = promptmutator(negative, distance)\n",
        "        negative = syn.mutate_specific_words()\n",
        "        opt.negative = negative\n",
        "    print(f\"Generating: {opt.prompt}\")\n",
        "    if pdist > 0:\n",
        "        print(f\"Distance: {pdist}\")\n",
        "    if opt.negative != \"\":\n",
        "        print(f\"Minus: {opt.negative}\")\n",
        "        if ndist > 0:\n",
        "          print(f\"Distance: {ndist}\")\n",
        "    images, filepaths = generate(opt)\n",
        "\n",
        "    print(f\"Used seed: {opt.seed}\")\n",
        "    print(f\"Saved to: {opt.outdir}\")\n",
        "    if not upscale_results:\n",
        "      for image in images:\n",
        "        display(image)\n",
        "    if upscale_results:    \n",
        "      print(\"Upscaling image\")\n",
        "      u_images, u_filepaths = to_big(filepaths, opt)\n",
        "      print(\"Upscaled:\")\n",
        "      for image in u_images:\n",
        "        display(image)\n",
        "    print(f\"Prompt: {opt.prompt}\")\n",
        "    if opt.negative != \"\":\n",
        "        print(f\"Negative: {opt.negative}\")\n",
        "    print(f\"Seed: {opt.seed}\")\n",
        "    \n",
        "    opt.seed = opt.seed + 1\n",
        "\n",
        "print(\"done\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "jxTGQnrCn9RX",
        "WR73Rwd-oU8n"
      ],
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.1 (tags/v3.11.1:a7a450f, Dec  6 2022, 19:58:39) [MSC v.1934 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "3dd66bf6c8e9f7cc366e2708e23aa0e93d8195bd81b4588014ee73c3f45a53a9"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
